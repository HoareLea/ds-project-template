# {{ cookiecutter.project_name }}
{{ cookiecutter.description }}

## Project Structure
```
â”œâ”€â”€ .env                   <- Local secrets and credentials that should not be stored in source control.
â”œâ”€â”€ Makefile               <- Makefile with useful commands for project setup and running analysis.
â”œâ”€â”€ README.md              <- The top-level README for developers using this project.
â”œâ”€â”€ app                    <- App-specific code, requirements file and Dockerfile.
â”œâ”€â”€ conf                   <- Configuration files that can be stored in source control.
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ 01_raw             <- The original, immutable data dump.
â”‚   â”œâ”€â”€ 02_intermediate    <- Intermediate data that has been transformed.
â”‚   â”œâ”€â”€ 03_model_input     <- The final, canonical data sets for modeling.
â”‚   â””â”€â”€ 04_model_output    <- Outputs from models (e.g. predictions).
â”œâ”€â”€ models                 <- Trained and serialized models or model summaries.
â”œâ”€â”€ notebooks              <- Jupyter notebooks.
â”œâ”€â”€ pipelines              <- Pipeline scripts for data processing and model training.
â”œâ”€â”€ pyproject.toml         <- Project metadata and dependencies.
â”œâ”€â”€ references             <- Data dictionaries, manuals, and all other explanatory materials.
â”œâ”€â”€ src                    <- Source code for use in this project.
â”‚   â””â”€â”€ {{ cookiecutter.package_name }}
â”‚       â”œâ”€â”€ __init__.py    <- Make {{ cookiecutter.package_name }} a Python module.
â”‚       â”œâ”€â”€ data           <- Scripts to download or generate data.
â”‚       â”œâ”€â”€ features       <- Scripts to turn raw data into features for modeling.
â”‚       â”œâ”€â”€ model          <- Scripts to train models and make predictions.
â”‚       â”œâ”€â”€ utils          <- Utility functions.
â”‚       â””â”€â”€ visualization  <- Scripts to create exploratory and results-oriented visualizations.
â””â”€â”€ tests                  <- Tests for functions in src.
```

## Getting Started

### Setup

1. **Create and Activate Virtual Environment**:
   ```bash
   python -m venv .venv
   source .venv/bin/activate
   ```

2. **Install Dependencies and Initialize Git**:
   ```bash
   git init
   pip install -U pip setuptools
   pip install -e .[dev]
   pre-commit install
   ```

   Alternatively, you can use the `make` command:
   ```bash
   git init
   make install
   ```

3. **Make Initial Commit**:
   ```bash
   git add .
   git commit -m "Initial commit"
   ```

### Usage

#### Data

- **Immutability**: Raw data should not be edited. Transform data through your processing pipeline.
- **Directory Structure**: Organize any local data into `01_raw`, `02_intermediate`, `03_model_input`, and `04_model_output`.

#### Pipelines

- **Prefect**: Prefect is the default orchestration tool but you are free to use whatever technology you like. If Prefect is selected:
   - **Task and Flow Definition**: Prefect tasks and flows are defined in `pipelines/tasks.py` and `pipelines/flows.py`.
   - **Execution**: Use the Makefile to spin up the Prefect server and manage pipelines.
 
#### Code Quality

- **Black**: Black is installed as a pre-commit hook and will automatically format any python code. This enables faster code review and small diffs.
- **Flake8**: Flake8 is used for linting and installed as a pre-commit hook.

#### Notebooks

- **Purpose**: Notebooks are for exploration and communication. Refactor useful code into the `src` directory.
- **nbstripout**: Notebook output should ~not~ rarely be committed to source control because it creates ugly diffs and risks data leakage. Nbstripout is installed as a pre-commit hook. It can be ignored by setting the ```"keep_output": true``` metadata on a cell.
- **Auto-reloading**:
  ```pythonðŸš¡
  %load_ext autoreload
  %autoreload 2
  ```

#### Applications

- **Streamlit and FastAPI**: If selected, templates are provided with `requirements.txt` and `Dockerfile` for building containerized apps.

### Cloud Storage and Database Connections

- **Cloud Storage**: If selected, utility functions for connecting to cloud storage are in `utils/cloud_storage.py`. Configuration settings are added to `.env`.
- **Database Connections**: If selected, utility functions for database connections are in `utils/db.py`. Configuration settings are added to `.env`.

### Project Philosophy

The goal is to maintain modularity and separation of concerns:
- **Shared Code**: All reusable code should reside in the `src/{{ cookiecutter.package_name }}` directory.
- **Apps, Pipelines, and Notebooks**: Use the shared code in apps, pipelines, and notebooks, ensuring that your project remains clean and maintainable.

## Acknowledgements

This project template is based on the [Data Science Boilerplate](https://github.com/andrewjkuo/ds-boilerplate), influenced by [Cookiecutter Data Science](https://drivendata.github.io/cookiecutter-data-science/), [Kedro](https://kedro.org/), and [govcookiecutter](https://best-practice-and-impact.github.io/govcookiecutter/#govcookiecutter).
